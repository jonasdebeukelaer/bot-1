# Generated by CodiumAI
import pytest

from src.bot.llm_interface import LLMInterface


class TestLLMInterface:

    # LLMInterface can be initialized with a valid model name.
    def test_initialized_with_valid_model_name(self):
        model_name = "valid_model"
        interface = LLMInterface(model_name)
        assert interface.model_name == model_name

    # LLMInterface raises ValueError if model name contains 'gpt' and OPENAI_API_KEY is not set in the environment variables.
    def test_raises_value_error_if_model_name_contains_gpt_and_openai_api_key_not_set(self, monkeypatch):
        model_name = "gpt_model"
        monkeypatch.delenv("OPENAI_API_KEY", raising=False)
        with pytest.raises(ValueError):
            LLMInterface(model_name)


## TODO: figure out how to mock response properly
# from openai.types.chat import ChatCompletionMessageToolCall
#     # send_messages method can successfully send messages to OpenAI API.
#     def test_send_messages_success(self):
#         # Initialize LLMInterface with a valid model name
#         model_name = "valid_model"
#         interface = LLMInterface(model_name)

#         # Define test data
#         messages = [
#             {"role": "system", "content": "initialize"},
#             {"role": "user", "content": "Hello, how are you?"},
#             {"role": "assistant", "content": "I'm doing well, thank you! How can I assist you today?"},
#         ]
#         tool = {"function": {"name": "tool_function", "arguments": {"arg1": "value1", "arg2": "value2"}}}

#         # Mock the litellm.completion function
#         def mock_completion(model, messages, tools, tool_choice):
#             return {
#                 "choices": [
#                     {
#                         "message": {
#                             "tool_calls": [
#                                 ChatCompletionMessageToolCall(
#                                     function=Function(arguments='{"response_arg1": "response_value1"}')
#                                 )
#                             ]
#                         }
#                     }
#                 ]
#             }

#         litellm.completion = mock_completion

#         # Call the send_messages method
#         response = interface.send_messages(messages, tool)

#         # Assert the response is as expected
#         assert response == {"response_arg1": "response_value1"}
